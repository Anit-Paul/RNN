{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyKtw6tb8Zan",
        "outputId": "43a02c97-049f-465a-df2a-fba947c84452"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     C:\\Users\\anit4\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EMTjrud58vKC"
      },
      "outputs": [],
      "source": [
        "data=gutenberg.raw('shakespeare-hamlet.txt')\n",
        "with open('hamlet.txt','w') as file:\n",
        "    file.write(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19Zss9nG9Asz",
        "outputId": "4e6342d9-d1fc-48c6-aa84-c6249b6fe17e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\RNN\\venv39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "4818\n"
          ]
        }
      ],
      "source": [
        "#data preprocessing\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer #for converting senteces into vector\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences #This function is used to make all sequences the same length by padding or truncating them.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#load the dataset\n",
        "with open('hamlet.txt','r') as file:\n",
        "    data=file.read().lower()\n",
        "\n",
        "#tokenize\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "total_words=len(tokenizer.word_index)+1\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5DjGxAjN-v3z"
      },
      "outputs": [],
      "source": [
        "#create input sequences\n",
        "inputSequences=[]\n",
        "text=data.split(\"\\n\")\n",
        "for i in text:\n",
        "    token_list=tokenizer.texts_to_sequences([i])[0]\n",
        "    for j in range(1,len(token_list)):\n",
        "        n_gram_sequence=token_list[:j+1]\n",
        "        inputSequences.append(n_gram_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XeKIrpu4AwOg"
      },
      "outputs": [],
      "source": [
        "#pad sequences\n",
        "max_sequence_len=max([len(x) for x in inputSequences])\n",
        "inputSequences=np.array(pad_sequences(inputSequences,maxlen=max_sequence_len,padding='pre'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cINSE4MdAg3i",
        "outputId": "c468e041-f514-42cc-8a76-92c1e7a8c255"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    0,    1,  687],\n",
              "       [   0,    0,    0, ...,    1,  687,    4],\n",
              "       [   0,    0,    0, ...,  687,    4,   45],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,    4,   45, 1047],\n",
              "       [   0,    0,    0, ...,   45, 1047,    4],\n",
              "       [   0,    0,    0, ..., 1047,    4,  193]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputSequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kUgmYAwqBFrJ"
      },
      "outputs": [],
      "source": [
        "#create predictors\n",
        "x,y=inputSequences[:,:-1],inputSequences[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JrU-FOmjBWjr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "y=tf.keras.utils.to_categorical(y,num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2Vb5RnXMDvEw"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKmjiJcVD6Mq"
      },
      "source": [
        "Train the lstm RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmLtB7YKD55A",
        "outputId": "50b05dd5-2fad-47b7-8db8-143f90bbbbe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\RNN\\venv39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
        "#Embedding : It converts word indexes (integers) into dense, learnable vectors of fixed size.\n",
        "model=Sequential()\n",
        "model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words,activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-xJagzimU1oQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\RNN\\venv39\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "uuXtGvd3U9cq",
        "outputId": "b7333166-a903-4339-be79-0dcd1b5dd1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 13, 100)           481800    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 13, 150)           150600    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 13, 150)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               100400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4818)              486618    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1219418 (4.65 MB)\n",
            "Trainable params: 1219418 (4.65 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hRXzMruVczr",
        "outputId": "2a869d5f-6828-4730-bdd4-66bc7bd8f635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "644/644 [==============================] - 22s 34ms/step - loss: 2.5953 - accuracy: 0.4293 - val_loss: 11.3171 - val_accuracy: 0.0528\n",
            "Epoch 2/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 2.5564 - accuracy: 0.4404 - val_loss: 11.4269 - val_accuracy: 0.0528\n",
            "Epoch 3/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 2.5311 - accuracy: 0.4441 - val_loss: 11.4865 - val_accuracy: 0.0528\n",
            "Epoch 4/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 2.4924 - accuracy: 0.4553 - val_loss: 11.5375 - val_accuracy: 0.0492\n",
            "Epoch 5/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 2.4612 - accuracy: 0.4551 - val_loss: 11.5804 - val_accuracy: 0.0527\n",
            "Epoch 6/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 2.4319 - accuracy: 0.4678 - val_loss: 11.6525 - val_accuracy: 0.0517\n",
            "Epoch 7/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 2.4087 - accuracy: 0.4679 - val_loss: 11.7274 - val_accuracy: 0.0507\n",
            "Epoch 8/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 2.3699 - accuracy: 0.4778 - val_loss: 11.7974 - val_accuracy: 0.0513\n",
            "Epoch 9/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 2.3448 - accuracy: 0.4797 - val_loss: 11.8588 - val_accuracy: 0.0530\n",
            "Epoch 10/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 2.3236 - accuracy: 0.4871 - val_loss: 11.8864 - val_accuracy: 0.0528\n",
            "Epoch 11/100\n",
            "644/644 [==============================] - 20s 31ms/step - loss: 2.2958 - accuracy: 0.4894 - val_loss: 11.9603 - val_accuracy: 0.0501\n",
            "Epoch 12/100\n",
            "644/644 [==============================] - 20s 31ms/step - loss: 2.2577 - accuracy: 0.5007 - val_loss: 12.0370 - val_accuracy: 0.0493\n",
            "Epoch 13/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 2.2376 - accuracy: 0.5009 - val_loss: 12.0926 - val_accuracy: 0.0523\n",
            "Epoch 14/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 2.2122 - accuracy: 0.5082 - val_loss: 12.1302 - val_accuracy: 0.0521\n",
            "Epoch 15/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 2.1873 - accuracy: 0.5112 - val_loss: 12.2276 - val_accuracy: 0.0525\n",
            "Epoch 16/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 2.1620 - accuracy: 0.5138 - val_loss: 12.2110 - val_accuracy: 0.0499\n",
            "Epoch 17/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 2.1413 - accuracy: 0.5217 - val_loss: 12.2959 - val_accuracy: 0.0528\n",
            "Epoch 18/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 2.1142 - accuracy: 0.5223 - val_loss: 12.3319 - val_accuracy: 0.0525\n",
            "Epoch 19/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 2.0953 - accuracy: 0.5317 - val_loss: 12.3835 - val_accuracy: 0.0509\n",
            "Epoch 20/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 2.0643 - accuracy: 0.5344 - val_loss: 12.4732 - val_accuracy: 0.0544\n",
            "Epoch 21/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 2.0437 - accuracy: 0.5382 - val_loss: 12.5230 - val_accuracy: 0.0521\n",
            "Epoch 22/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 2.0188 - accuracy: 0.5486 - val_loss: 12.6053 - val_accuracy: 0.0536\n",
            "Epoch 23/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.9981 - accuracy: 0.5479 - val_loss: 12.5712 - val_accuracy: 0.0509\n",
            "Epoch 24/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.9698 - accuracy: 0.5560 - val_loss: 12.6746 - val_accuracy: 0.0527\n",
            "Epoch 25/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.9656 - accuracy: 0.5566 - val_loss: 12.7569 - val_accuracy: 0.0530\n",
            "Epoch 26/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.9475 - accuracy: 0.5586 - val_loss: 12.7613 - val_accuracy: 0.0528\n",
            "Epoch 27/100\n",
            "644/644 [==============================] - 20s 30ms/step - loss: 1.9140 - accuracy: 0.5653 - val_loss: 12.8342 - val_accuracy: 0.0527\n",
            "Epoch 28/100\n",
            "644/644 [==============================] - 22s 34ms/step - loss: 1.8890 - accuracy: 0.5735 - val_loss: 12.8932 - val_accuracy: 0.0507\n",
            "Epoch 29/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.8691 - accuracy: 0.5787 - val_loss: 12.9310 - val_accuracy: 0.0509\n",
            "Epoch 30/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.8550 - accuracy: 0.5786 - val_loss: 12.9697 - val_accuracy: 0.0507\n",
            "Epoch 31/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.8367 - accuracy: 0.5828 - val_loss: 13.0080 - val_accuracy: 0.0490\n",
            "Epoch 32/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 1.8224 - accuracy: 0.5850 - val_loss: 13.0369 - val_accuracy: 0.0490\n",
            "Epoch 33/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.8012 - accuracy: 0.5890 - val_loss: 13.1040 - val_accuracy: 0.0495\n",
            "Epoch 34/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.7835 - accuracy: 0.5920 - val_loss: 13.1670 - val_accuracy: 0.0499\n",
            "Epoch 35/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.7642 - accuracy: 0.5958 - val_loss: 13.2429 - val_accuracy: 0.0521\n",
            "Epoch 36/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 1.7514 - accuracy: 0.5996 - val_loss: 13.2240 - val_accuracy: 0.0513\n",
            "Epoch 37/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 1.7285 - accuracy: 0.6034 - val_loss: 13.3211 - val_accuracy: 0.0513\n",
            "Epoch 38/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 1.7149 - accuracy: 0.6062 - val_loss: 13.3383 - val_accuracy: 0.0511\n",
            "Epoch 39/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 1.6988 - accuracy: 0.6134 - val_loss: 13.4017 - val_accuracy: 0.0501\n",
            "Epoch 40/100\n",
            "644/644 [==============================] - 19s 30ms/step - loss: 1.6816 - accuracy: 0.6160 - val_loss: 13.4632 - val_accuracy: 0.0499\n",
            "Epoch 41/100\n",
            "644/644 [==============================] - 22s 33ms/step - loss: 1.6701 - accuracy: 0.6193 - val_loss: 13.5083 - val_accuracy: 0.0507\n",
            "Epoch 42/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.6489 - accuracy: 0.6221 - val_loss: 13.5274 - val_accuracy: 0.0509\n",
            "Epoch 43/100\n",
            "644/644 [==============================] - 22s 34ms/step - loss: 1.6297 - accuracy: 0.6274 - val_loss: 13.5744 - val_accuracy: 0.0474\n",
            "Epoch 44/100\n",
            "644/644 [==============================] - 23s 36ms/step - loss: 1.6299 - accuracy: 0.6273 - val_loss: 13.6404 - val_accuracy: 0.0499\n",
            "Epoch 45/100\n",
            "644/644 [==============================] - 23s 36ms/step - loss: 1.6040 - accuracy: 0.6334 - val_loss: 13.6791 - val_accuracy: 0.0484\n",
            "Epoch 46/100\n",
            "644/644 [==============================] - 22s 34ms/step - loss: 1.5820 - accuracy: 0.6338 - val_loss: 13.7017 - val_accuracy: 0.0509\n",
            "Epoch 47/100\n",
            "644/644 [==============================] - 22s 35ms/step - loss: 1.5793 - accuracy: 0.6357 - val_loss: 13.7872 - val_accuracy: 0.0478\n",
            "Epoch 48/100\n",
            "644/644 [==============================] - 23s 36ms/step - loss: 1.5720 - accuracy: 0.6394 - val_loss: 13.8020 - val_accuracy: 0.0499\n",
            "Epoch 49/100\n",
            "644/644 [==============================] - 22s 34ms/step - loss: 1.5540 - accuracy: 0.6398 - val_loss: 13.8264 - val_accuracy: 0.0505\n",
            "Epoch 50/100\n",
            "644/644 [==============================] - 22s 34ms/step - loss: 1.5374 - accuracy: 0.6430 - val_loss: 13.8908 - val_accuracy: 0.0505\n",
            "Epoch 51/100\n",
            "644/644 [==============================] - 22s 35ms/step - loss: 1.5234 - accuracy: 0.6460 - val_loss: 13.9138 - val_accuracy: 0.0499\n",
            "Epoch 52/100\n",
            "644/644 [==============================] - 22s 34ms/step - loss: 1.5125 - accuracy: 0.6530 - val_loss: 13.9597 - val_accuracy: 0.0503\n",
            "Epoch 53/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.4890 - accuracy: 0.6549 - val_loss: 14.0326 - val_accuracy: 0.0519\n",
            "Epoch 54/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.4826 - accuracy: 0.6589 - val_loss: 14.0752 - val_accuracy: 0.0505\n",
            "Epoch 55/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.4717 - accuracy: 0.6579 - val_loss: 14.1097 - val_accuracy: 0.0503\n",
            "Epoch 56/100\n",
            "644/644 [==============================] - 22s 34ms/step - loss: 1.4636 - accuracy: 0.6597 - val_loss: 14.1160 - val_accuracy: 0.0499\n",
            "Epoch 57/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 1.4560 - accuracy: 0.6611 - val_loss: 14.1525 - val_accuracy: 0.0501\n",
            "Epoch 58/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.4387 - accuracy: 0.6665 - val_loss: 14.2367 - val_accuracy: 0.0530\n",
            "Epoch 59/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.4208 - accuracy: 0.6712 - val_loss: 14.2504 - val_accuracy: 0.0499\n",
            "Epoch 60/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.4052 - accuracy: 0.6761 - val_loss: 14.3153 - val_accuracy: 0.0490\n",
            "Epoch 61/100\n",
            "644/644 [==============================] - 23s 36ms/step - loss: 1.4064 - accuracy: 0.6737 - val_loss: 14.3406 - val_accuracy: 0.0511\n",
            "Epoch 62/100\n",
            "644/644 [==============================] - 29s 45ms/step - loss: 1.3921 - accuracy: 0.6762 - val_loss: 14.3784 - val_accuracy: 0.0480\n",
            "Epoch 63/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.3783 - accuracy: 0.6802 - val_loss: 14.4514 - val_accuracy: 0.0509\n",
            "Epoch 64/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 1.3706 - accuracy: 0.6837 - val_loss: 14.4337 - val_accuracy: 0.0501\n",
            "Epoch 65/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 1.3516 - accuracy: 0.6864 - val_loss: 14.4598 - val_accuracy: 0.0507\n",
            "Epoch 66/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.3386 - accuracy: 0.6925 - val_loss: 14.5361 - val_accuracy: 0.0484\n",
            "Epoch 67/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.3295 - accuracy: 0.6918 - val_loss: 14.5434 - val_accuracy: 0.0517\n",
            "Epoch 68/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.3250 - accuracy: 0.6927 - val_loss: 14.6229 - val_accuracy: 0.0501\n",
            "Epoch 69/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.3135 - accuracy: 0.6926 - val_loss: 14.6592 - val_accuracy: 0.0509\n",
            "Epoch 70/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.3099 - accuracy: 0.6926 - val_loss: 14.7008 - val_accuracy: 0.0513\n",
            "Epoch 71/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.2967 - accuracy: 0.6976 - val_loss: 14.7640 - val_accuracy: 0.0462\n",
            "Epoch 72/100\n",
            "644/644 [==============================] - 20s 30ms/step - loss: 1.2853 - accuracy: 0.7019 - val_loss: 14.7878 - val_accuracy: 0.0521\n",
            "Epoch 73/100\n",
            "644/644 [==============================] - 22s 33ms/step - loss: 1.2800 - accuracy: 0.7009 - val_loss: 14.7671 - val_accuracy: 0.0499\n",
            "Epoch 74/100\n",
            "644/644 [==============================] - 22s 34ms/step - loss: 1.2662 - accuracy: 0.7045 - val_loss: 14.8400 - val_accuracy: 0.0503\n",
            "Epoch 75/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.2589 - accuracy: 0.7060 - val_loss: 14.8562 - val_accuracy: 0.0503\n",
            "Epoch 76/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.2547 - accuracy: 0.7068 - val_loss: 14.9000 - val_accuracy: 0.0472\n",
            "Epoch 77/100\n",
            "644/644 [==============================] - 22s 34ms/step - loss: 1.2405 - accuracy: 0.7086 - val_loss: 14.9401 - val_accuracy: 0.0499\n",
            "Epoch 78/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.2322 - accuracy: 0.7115 - val_loss: 14.9685 - val_accuracy: 0.0484\n",
            "Epoch 79/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.2291 - accuracy: 0.7132 - val_loss: 14.9818 - val_accuracy: 0.0492\n",
            "Epoch 80/100\n",
            "644/644 [==============================] - 22s 33ms/step - loss: 1.2216 - accuracy: 0.7142 - val_loss: 15.0333 - val_accuracy: 0.0490\n",
            "Epoch 81/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.2076 - accuracy: 0.7165 - val_loss: 15.0739 - val_accuracy: 0.0482\n",
            "Epoch 82/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.2025 - accuracy: 0.7172 - val_loss: 15.1337 - val_accuracy: 0.0515\n",
            "Epoch 83/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.1864 - accuracy: 0.7219 - val_loss: 15.1499 - val_accuracy: 0.0495\n",
            "Epoch 84/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.1765 - accuracy: 0.7237 - val_loss: 15.1747 - val_accuracy: 0.0468\n",
            "Epoch 85/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 1.1803 - accuracy: 0.7224 - val_loss: 15.1818 - val_accuracy: 0.0517\n",
            "Epoch 86/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 1.1722 - accuracy: 0.7222 - val_loss: 15.2544 - val_accuracy: 0.0495\n",
            "Epoch 87/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.1670 - accuracy: 0.7237 - val_loss: 15.2589 - val_accuracy: 0.0474\n",
            "Epoch 88/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.1552 - accuracy: 0.7262 - val_loss: 15.2826 - val_accuracy: 0.0457\n",
            "Epoch 89/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.1445 - accuracy: 0.7281 - val_loss: 15.3397 - val_accuracy: 0.0488\n",
            "Epoch 90/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.1475 - accuracy: 0.7287 - val_loss: 15.3950 - val_accuracy: 0.0492\n",
            "Epoch 91/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.1336 - accuracy: 0.7313 - val_loss: 15.4355 - val_accuracy: 0.0468\n",
            "Epoch 92/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.1298 - accuracy: 0.7323 - val_loss: 15.4385 - val_accuracy: 0.0517\n",
            "Epoch 93/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.1220 - accuracy: 0.7358 - val_loss: 15.4777 - val_accuracy: 0.0492\n",
            "Epoch 94/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.1095 - accuracy: 0.7360 - val_loss: 15.5509 - val_accuracy: 0.0486\n",
            "Epoch 95/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.1058 - accuracy: 0.7376 - val_loss: 15.4771 - val_accuracy: 0.0488\n",
            "Epoch 96/100\n",
            "644/644 [==============================] - 21s 33ms/step - loss: 1.1010 - accuracy: 0.7371 - val_loss: 15.5485 - val_accuracy: 0.0468\n",
            "Epoch 97/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 1.0954 - accuracy: 0.7376 - val_loss: 15.5993 - val_accuracy: 0.0488\n",
            "Epoch 98/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 1.0940 - accuracy: 0.7395 - val_loss: 15.5582 - val_accuracy: 0.0484\n",
            "Epoch 99/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 1.0902 - accuracy: 0.7390 - val_loss: 15.6083 - val_accuracy: 0.0480\n",
            "Epoch 100/100\n",
            "644/644 [==============================] - 21s 32ms/step - loss: 1.0797 - accuracy: 0.7418 - val_loss: 15.7387 - val_accuracy: 0.0493\n"
          ]
        }
      ],
      "source": [
        "history=model.fit(x_train,y_train,epochs=100,verbose=1,validation_data=(x_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HFH8HpoMnGOO"
      },
      "outputs": [],
      "source": [
        "#function to predict the next word\n",
        "def predict_next_word(model,tokenizer,text,max_sequence_length):\n",
        "    token_list=tokenizer.texts_to_sequences([text])[0]\n",
        "    token_list=pad_sequences([token_list],maxlen=max_sequence_length-1,padding='pre')\n",
        "    predicted=model.predict(token_list,verbose=0)\n",
        "    predicted=np.argmax(predicted,axis=1)\n",
        "    for word,index in tokenizer.word_index.items():\n",
        "        if index==predicted:\n",
        "            return word\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KHiNEHVDnyIn",
        "outputId": "2bd24799-ed63-48ad-b736-df7b9f02566b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'bearers'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_text=\"i am a\"\n",
        "predict_next_word(model,tokenizer,input_text,max_sequence_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raJ_A1ddwq1F",
        "outputId": "be05e3f9-10e9-4379-e84e-2ad76b57902e"
      },
      "outputs": [],
      "source": [
        "#save the model\n",
        "model.save('lstm_model.h5')\n",
        "#save the tokenizer\n",
        "import pickle\n",
        "with open('tokenizer.pickle','wb') as handle:\n",
        "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
